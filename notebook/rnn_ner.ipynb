{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5458e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import thư viện\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6582d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bead3e244ab422aac624b1a1728edd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003/train/0000.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DoubleDD\\VSC_Workspace\\VSCode_Python\\common-venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DoubleDD\\.cache\\huggingface\\hub\\datasets--conll2003. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494edcf113c54f3c823359d65e5f7bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/312k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1661e1be8f1a4c2f874e224de338f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f546ca4a4b064b7ea98cc7b131e88b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cc6e3da5914a8d8014a94dac68077e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5ecf018ba94cbbb5b6d5995c59c1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tải dữ liệu CoNLL 2003\n",
    "dataset = load_dataset(\"conll2003\", revision=\"refs/convert/parquet\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda1b7c",
   "metadata": {},
   "source": [
    "#### Task 1.2: Trích xuất câu và nhãn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe7c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Lấy mapping id -> tag string\n",
    "tag_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "# Train\n",
    "train_sentences = dataset[\"train\"][\"tokens\"]\n",
    "train_tags_id = dataset[\"train\"][\"ner_tags\"]\n",
    "train_tags = [[tag_names[tag] for tag in sent] for sent in train_tags_id]\n",
    "\n",
    "# Validation\n",
    "val_sentences = dataset[\"validation\"][\"tokens\"]\n",
    "val_tags_id = dataset[\"validation\"][\"ner_tags\"]\n",
    "val_tags = [[tag_names[tag] for tag in sent] for sent in val_tags_id]\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_tags[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b730be0",
   "metadata": {},
   "source": [
    "#### Task 1.3: Xây dựng Vocabulary (word_to_ix, tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0f0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23625\n",
      "Number of NER tags: 9\n"
     ]
    }
   ],
   "source": [
    "# -------- WORD VOCAB --------\n",
    "word_counter = Counter()\n",
    "for sent in train_sentences:\n",
    "    word_counter.update(sent)\n",
    "\n",
    "word_to_ix = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1\n",
    "}\n",
    "\n",
    "for word in word_counter:\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# -------- TAG VOCAB --------\n",
    "tag_to_ix = {}\n",
    "for sent in train_tags:\n",
    "    for tag in sent:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "print(\"Vocabulary size:\", len(word_to_ix))\n",
    "print(\"Number of NER tags:\", len(tag_to_ix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b113608",
   "metadata": {},
   "source": [
    "#### Task 2.1: Dataset cho NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214da84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "\n",
    "        sent_idx = [\n",
    "            self.word_to_ix.get(word, self.word_to_ix[\"<UNK>\"])\n",
    "            for word in sentence\n",
    "        ]\n",
    "\n",
    "        tag_idx = [\n",
    "            self.tag_to_ix[tag] for tag in tags\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(sent_idx), torch.tensor(tag_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b11de",
   "metadata": {},
   "source": [
    "#### Task 2.2: Collate function + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505fd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD_IDX = word_to_ix[\"<PAD>\"]\n",
    "PAD_TAG_IDX = -1  # ignore_index cho loss\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "\n",
    "    sentences_padded = pad_sequence(\n",
    "        sentences, batch_first=True, padding_value=PAD_WORD_IDX\n",
    "    )\n",
    "\n",
    "    tags_padded = pad_sequence(\n",
    "        tags, batch_first=True, padding_value=PAD_TAG_IDX\n",
    "    )\n",
    "\n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
    "val_dataset = NERDataset(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d977ff",
   "metadata": {},
   "source": [
    "#### Task 3: Mô hình RNN cho Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d9d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNForNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=PAD_WORD_IDX\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim, hidden_dim, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)              # (B, T, E)\n",
    "        out, _ = self.rnn(emb)               # (B, T, H)\n",
    "        logits = self.fc(out)                # (B, T, C)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf0f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleRNNForNER(\n",
    "    vocab_size=len(word_to_ix),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    num_tags=len(tag_to_ix)\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TAG_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11e228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm huấn luyện\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for sentences, tags in loader:\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(sentences)\n",
    "        loss = criterion(\n",
    "            outputs.view(-1, outputs.shape[-1]),\n",
    "            tags.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Hàm đánh giá\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in loader:\n",
    "            sentences = sentences.to(device)\n",
    "            tags = tags.to(device)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            mask = tags != PAD_TAG_IDX\n",
    "            correct += ((predictions == tags) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c7973",
   "metadata": {},
   "source": [
    "#### Task 4: Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b2ba0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6412\n",
      "Validation Accuracy: 0.8699\n",
      "----------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3720\n",
      "Validation Accuracy: 0.8992\n",
      "----------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.2572\n",
      "Validation Accuracy: 0.9119\n",
      "----------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1859\n",
      "Validation Accuracy: 0.9278\n",
      "----------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.1381\n",
      "Validation Accuracy: 0.9305\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff73438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.N.            -> B-ORG\n",
      "official        -> O\n",
      "Ekeus           -> B-ORG\n",
      "heads           -> O\n",
      "for             -> O\n",
      "Baghdad         -> B-LOC\n",
      ".               -> O\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán cho 1 câu mới và test nhanh\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = sentence.split()\n",
    "    indices = [\n",
    "        word_to_ix.get(word, word_to_ix[\"<UNK>\"])\n",
    "        for word in tokens\n",
    "    ]\n",
    "\n",
    "    x = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=-1).squeeze(0)\n",
    "\n",
    "    for word, tag_idx in zip(tokens, preds):\n",
    "        print(f\"{word:15} -> {ix_to_tag[tag_idx.item()]}\")\n",
    "\n",
    "\n",
    "predict_sentence(\"U.N. official Ekeus heads for Baghdad .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09881b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
