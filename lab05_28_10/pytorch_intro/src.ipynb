{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8113ac5a",
   "metadata": {},
   "source": [
    "PHẦN 1: KHÁM PHÁ TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb96911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor từ list:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Tensor từ NumPy array:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "\n",
      "Ones Tensor:\n",
      " tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Random Tensor:\n",
      " tensor([[0.2649, 0.8935],\n",
      "        [0.7887, 0.5429]])\n",
      "\n",
      "Shape của tensor: torch.Size([2, 2])\n",
      "Datatype của tensor: torch.float32\n",
      "Device lưu trữ tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Tạo tensor từ list\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor từ list:\\n {x_data}\\n\")\n",
    "\n",
    "# Tạo tensor từ NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor từ NumPy array:\\n {x_np}\\n\")\n",
    "\n",
    "# Tạo tensor với các giá trị ngẫu nhiên hoặc hằng số\n",
    "x_ones = torch.ones_like(x_data) # tạo tensor gồm các số 1 có cùng shape với x_data\n",
    "print(f\"Ones Tensor:\\n {x_ones}\\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # tạo tensor ngẫu nhiên\n",
    "print(f\"Random Tensor:\\n {x_rand}\\n\")\n",
    "\n",
    "# In ra shape, dtype, và device của tensor\n",
    "print(f\"Shape của tensor: {x_rand.shape}\")\n",
    "print(f\"Datatype của tensor: {x_rand.dtype}\")\n",
    "print(f\"Device lưu trữ tensor: {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9c846",
   "metadata": {},
   "source": [
    "TASK 1.2: CÁC PHÉP TOÁN TRÊN TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1098cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 1.2: Các phép toán trên Tensor ===\n",
      "Cộng x_data với chính nó:\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "\n",
      "Nhân x_data với 5:\n",
      "tensor([[ 5, 10],\n",
      "        [15, 20]])\n",
      "\n",
      "Nhân ma trận x_data @ x_data.T:\n",
      "tensor([[ 5, 11],\n",
      "        [11, 25]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Task 1.2: Các phép toán trên Tensor ===\")\n",
    "\n",
    "# 1. Cộng x_data với chính nó\n",
    "add_result = x_data + x_data\n",
    "print(f\"Cộng x_data với chính nó:\\n{add_result}\\n\")\n",
    "\n",
    "# 2. Nhân x_data với 5\n",
    "mul_result = x_data * 5\n",
    "print(f\"Nhân x_data với 5:\\n{mul_result}\\n\")\n",
    "\n",
    "# 3. Nhân ma trận x_data với x_data.T\n",
    "matmul_result = x_data @ x_data.T\n",
    "print(f\"Nhân ma trận x_data @ x_data.T:\\n{matmul_result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481d11c",
   "metadata": {},
   "source": [
    "TASK 1.3 – INDEXING VÀ SLICING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb605ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 1.3: Indexing và Slicing ===\n",
      "Hàng đầu tiên:\n",
      "tensor([1, 2])\n",
      "\n",
      "Cột thứ hai:\n",
      "tensor([2, 4])\n",
      "\n",
      "Giá trị tại hàng 2, cột 2: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Task 1.3: Indexing và Slicing ===\")\n",
    "\n",
    "# 1. Lấy hàng đầu tiên\n",
    "row1 = x_data[0]\n",
    "print(f\"Hàng đầu tiên:\\n{row1}\\n\")\n",
    "\n",
    "# 2. Lấy cột thứ hai\n",
    "col2 = x_data[:, 1]\n",
    "print(f\"Cột thứ hai:\\n{col2}\\n\")\n",
    "\n",
    "# 3. Lấy giá trị hàng 2 cột 2\n",
    "value_22 = x_data[1, 1]\n",
    "print(f\"Giá trị tại hàng 2, cột 2: {value_22}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e04923",
   "metadata": {},
   "source": [
    "TASK 1.4 – THAY ĐỔI HÌNH DẠNG TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd404ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 1.4: Thay đổi hình dạng Tensor ===\n",
      "Tensor random ban đầu (4,4):\n",
      "tensor([[0.9947, 0.0322, 0.0368, 0.4303],\n",
      "        [0.2152, 0.5746, 0.5584, 0.3634],\n",
      "        [0.3612, 0.0082, 0.3496, 0.8010],\n",
      "        [0.1900, 0.0725, 0.7435, 0.5033]])\n",
      "\n",
      "Tensor sau khi reshape thành (16,1):\n",
      "tensor([[0.9947],\n",
      "        [0.0322],\n",
      "        [0.0368],\n",
      "        [0.4303],\n",
      "        [0.2152],\n",
      "        [0.5746],\n",
      "        [0.5584],\n",
      "        [0.3634],\n",
      "        [0.3612],\n",
      "        [0.0082],\n",
      "        [0.3496],\n",
      "        [0.8010],\n",
      "        [0.1900],\n",
      "        [0.0725],\n",
      "        [0.7435],\n",
      "        [0.5033]])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Task 1.4: Thay đổi hình dạng Tensor ===\")\n",
    "\n",
    "# Tạo tensor random shape (4, 4)\n",
    "rand_tensor = torch.rand(4, 4)\n",
    "print(f\"Tensor random ban đầu (4,4):\\n{rand_tensor}\\n\")\n",
    "\n",
    "# Reshape thành (16, 1)\n",
    "reshaped_tensor = rand_tensor.view(16, 1)  # hoặc rand_tensor.reshape(16, 1)\n",
    "print(f\"Tensor sau khi reshape thành (16,1):\\n{reshaped_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1f872",
   "metadata": {},
   "source": [
    "PHẦN 2: TỰ ĐỘNG TÍNH ĐẠO HÀM VỚI `autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24242151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1.], requires_grad=True)\n",
      "y: tensor([3.], grad_fn=<AddBackward0>)\n",
      "grad_fn của y: <AddBackward0 object at 0x000001E77376FB80>\n",
      "Đạo hàm của z theo x: tensor([18.])\n"
     ]
    }
   ],
   "source": [
    "# Tạo một tensor và yêu cầu tính đạo hàm cho nó\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# Thực hiện một phép toán\n",
    "y = x + 2\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# y được tạo ra từ một phép toán có x, nên nó cũng có grad_fn\n",
    "print(f\"grad_fn của y: {y.grad_fn}\")\n",
    "\n",
    "# Thực hiện thêm các phép toán\n",
    "z = y * y * 3\n",
    "\n",
    "# Tính đạo hàm của z theo x\n",
    "z.backward() # tương đương z.backward(torch.tensor(1.))\n",
    "\n",
    "# Đạo hàm được lưu trong thuộc tính .grad\n",
    "# Ta có z = 3 * (x+2)^2 => dz/dx = 6 * (x+2). Với x=1, dz/dx = 18\n",
    "print(f\"Đạo hàm của z theo x: {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c370be3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# QUESTION: Thử gọi z.backward() thêm 1 lần\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DoubleDD\\VSC_Workspace\\VSCode_Python\\common-venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DoubleDD\\VSC_Workspace\\VSCode_Python\\common-venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DoubleDD\\VSC_Workspace\\VSCode_Python\\common-venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# QUESTION: Thử gọi z.backward() thêm 1 lần\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHẬN XÉT: Mặc định, PyTorch giải phóng computational graph sau khi thực hiện .backward() nhằm tiết kiệm bộ nhớ\n",
    "# trừ khi có yêu cầu giữ lại, z.backward(retain_graph=True)\n",
    "# => Nhưng gradient sẽ cộng dồn, không reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a7481",
   "metadata": {},
   "source": [
    "PHẦN 3: XÂY DỰNG MÔ HÌNH ĐẦU TIÊN VỚI `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721f95e",
   "metadata": {},
   "source": [
    "TASK 3.1: LỚP `nn.linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa79b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 5])\n",
      "Output shape: torch.Size([3, 2])\n",
      "Original tensor:\n",
      " tensor([[-0.4312, -1.3338,  0.2170, -0.5862, -0.0319],\n",
      "        [-1.1398,  1.1832, -0.1005, -1.6019, -1.4577],\n",
      "        [ 0.5651, -0.1061, -0.0878, -0.9836,  0.4954]])\n",
      "Output:\n",
      " tensor([[ 0.4220, -0.3629],\n",
      "        [-0.3322, -0.0946],\n",
      "        [ 0.0060, -0.0544]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Lớp nn.Linear thực hiện một phép biến đổi tuyến tính y = xA^T + b.\n",
    "# Khởi tạo một lớp Linear biến đổi từ 5 chiều -> 2 chiều\n",
    "linear_layer = torch.nn.Linear(in_features=5, out_features=2)\n",
    "\n",
    "# Tạo một tensor đầu vào mẫu\n",
    "input_tensor = torch.randn(3, 5) # 3 mẫu, mỗi mẫu 5 chiều\n",
    "\n",
    "# Truyền đầu vào qua lớp linear\n",
    "output = linear_layer(input_tensor)\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Original tensor:\\n {input_tensor}\")\n",
    "print(f\"Output:\\n {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12268157",
   "metadata": {},
   "source": [
    "TASK 3.2: LỚP `nn.embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8525b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4])\n",
      "Output shape: torch.Size([4, 3])\n",
      "Embeddings:\n",
      " tensor([[ 0.7008, -0.1423, -0.5862],\n",
      "        [ 0.3810,  0.2561,  1.7654],\n",
      "        [-0.7272,  0.8784,  0.1362],\n",
      "        [ 2.4089, -0.2594,  0.5800]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Lớp nn.Embedding là một bảng tra cứu, dùng để ánh xạ các chỉ số của từ thành các vector embedding.\n",
    "\n",
    "# Khởi tạo lớp Embedding cho một từ điển 10 từ, mỗi từ biểu diễn bằng vector 3 chiều\n",
    "embedding_layer = torch.nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
    "\n",
    "# Tạo một tensor đầu vào chứa các chỉ số của từ (ví dụ: một câu)\n",
    "# Các chỉ số phải nhỏ hơn 10\n",
    "input_indices = torch.LongTensor([1, 5, 0, 8])\n",
    "\n",
    "# Lấy ra các vector embedding tương ứng\n",
    "embeddings = embedding_layer(input_indices)\n",
    "\n",
    "print(f\"Input shape: {input_indices.shape}\")\n",
    "print(f\"Output shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings:\\n {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979aca98",
   "metadata": {},
   "source": [
    "TASK 3.3: KẾT HỢP THÀNH MỘT `nn.module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d1d717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([1, 4, 2])\n",
      "Output:\n",
      " tensor([[[-0.2513,  0.8019],\n",
      "         [-0.0335,  0.1960],\n",
      "         [ 0.0429,  0.1761],\n",
      "         [-0.1017,  0.2990]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyFirstModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(MyFirstModel, self).__init__()\n",
    "        \n",
    "        # Định nghĩa các lớp (layer) bạn sẽ dùng\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()   # Hàm kích hoạt\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, indices):\n",
    "        # 1. Lấy embedding\n",
    "        embeds = self.embedding(indices)         # (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # 2. Truyền qua lớp linear và activation\n",
    "        hidden = self.activation(self.linear(embeds))  # áp dụng theo từng bước thời gian\n",
    "\n",
    "        # 3. Truyền qua lớp output\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Khởi tạo và kiểm tra mô hình\n",
    "model = MyFirstModel(\n",
    "    vocab_size=100,\n",
    "    embedding_dim=16,\n",
    "    hidden_dim=8,\n",
    "    output_dim=2\n",
    ")\n",
    "\n",
    "input_data = torch.LongTensor([[1, 2, 5, 9]])  # một câu gồm 4 từ (batch size = 1)\n",
    "output_data = model(input_data)\n",
    "\n",
    "print(\"Model output shape:\", output_data.shape)\n",
    "print(\"Output:\\n\", output_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
